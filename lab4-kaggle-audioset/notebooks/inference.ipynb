{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sazerlife/projects/courses/itmo/semester-2/event_detection/lab4-kaggle-audioset\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "from scipy import stats as st\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchaudio.transforms as T\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "from torchlibrosa.stft import LogmelFilterBank, Spectrogram\n",
    "from torchmetrics.classification import MultilabelAccuracy, MultilabelF1Score, Accuracy, F1Score\n",
    "from torchvision.transforms import Compose\n",
    "from tqdm import tqdm\n",
    "from transformers import ASTConfig, ASTFeatureExtractor, ASTModel\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from src.utils.train_val_split import train_val_split\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "SEED=12345\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "SAMPLE_RATE = 16000\n",
    "DATA_PATH = Path(\"data/raw/\")\n",
    "\n",
    "train_csv_path = DATA_PATH / \"train.csv\"\n",
    "train_audio_path = DATA_PATH / \"audio_train\"\n",
    "\n",
    "test_csv_path = DATA_PATH / \"test.csv\"\n",
    "test_audio_path = DATA_PATH /  \"audio_test\"\n",
    "\n",
    "\n",
    "EXPERIMENTS_PATH = Path(\"experiments/resnet34/\")\n",
    "submission_csv_path = EXPERIMENTS_PATH / \"submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferWrapper:\n",
    "    def __get_resnet34(self) -> nn.Module:\n",
    "        resnet_model = resnet34(pretrained=False)\n",
    "        resnet_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        resnet_model.fc = nn.Linear(512, 41)\n",
    "\n",
    "        state_dict = torch.load(EXPERIMENTS_PATH / \"checkpoint-55-epoch.pt\")\n",
    "        resnet_model.load_state_dict(state_dict)\n",
    "        resnet_model = resnet_model.to(DEVICE)\n",
    "\n",
    "        return resnet_model\n",
    "\n",
    "    def __init__(self, sample_rate=16000, n_fft = 1024, win_length = None, hop_length = 512, n_mels = 128) -> None:\n",
    "        self.melspec_transform = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            power=2.0,\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            n_mels=n_mels,\n",
    "            mel_scale=\"htk\",\n",
    "            normalized=True,\n",
    "        )\n",
    "        self.model = self.__get_resnet34()\n",
    "\n",
    "        labels = [\n",
    "            'Acoustic_guitar', 'Applause', 'Bark', 'Bass_drum', 'Burping_or_eructation', 'Bus', 'Cello', \n",
    "            'Chime', 'Clarinet', 'Computer_keyboard', 'Cough', 'Cowbell', 'Double_bass', 'Drawer_open_or_close', \n",
    "            'Electric_piano', 'Fart', 'Finger_snapping', 'Fireworks', 'Flute', 'Glockenspiel', 'Gong',\n",
    "            'Gunshot_or_gunfire', 'Harmonica', 'Hi-hat', 'Keys_jangling', 'Knock', 'Laughter', 'Meow', 'Microwave_oven', \n",
    "            'Oboe', 'Saxophone', 'Scissors', 'Shatter', 'Snare_drum', 'Squeak', 'Tambourine', 'Tearing', 'Telephone',\n",
    "            'Trumpet', 'Violin_or_fiddle', 'Writing'\n",
    "        ]\n",
    "        self.labels = {k: v for k, v in enumerate(labels)}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def infer_frame(self, audio: torch.FloatTensor):\n",
    "        melspectrogram = self.melspec_transform(audio).unsqueeze(0)\n",
    "        prediction = self.model(melspectrogram.to(DEVICE))\n",
    "        return torch.softmax(prediction[0], dim=-1).cpu()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, audio_path: str) -> str:\n",
    "        audio, sr = sf.read(audio_path, dtype=\"float32\", always_2d=True)\n",
    "        assert sr == SAMPLE_RATE\n",
    "        audio = torch.from_numpy(audio.T)\n",
    "        \n",
    "        if audio.shape[-1] / sr <= 2.0:\n",
    "            audio_padded = torch.zeros(1, sr * 5, dtype=audio.dtype)\n",
    "            audio_padded[:, :audio.shape[-1]] = audio[:, :]\n",
    "            prediction = self.infer_frame(audio_padded)\n",
    "        \n",
    "        elif audio.shape[-1] / sr < 10.0:\n",
    "            prediction = self.infer_frame(audio)\n",
    "        \n",
    "        else:\n",
    "            win_len = 5 * sr\n",
    "            hop_len = sr\n",
    "\n",
    "            padding_size = win_len - (audio.shape[-1] % hop_len)\n",
    "            audio_padded = torch.zeros(1, audio.shape[-1] + padding_size, dtype=audio.dtype)\n",
    "            audio_padded[:, :audio.shape[-1]] = audio[:, :]\n",
    "            audio = audio_padded\n",
    "            \n",
    "            predictions = list()\n",
    "            for idx in range(0, audio.shape[-1], hop_len):\n",
    "                prediction = self.infer_frame(audio[:, idx : idx + win_len])\n",
    "                # predictions.append(torch.round(prediction, decimals=2))\n",
    "                if prediction.max() > 0.15:\n",
    "                    predictions.append(prediction)\n",
    "\n",
    "            # for pred in predictions:\n",
    "            #     print(pred.max(), end=\", \")\n",
    "            # print()\n",
    "            # print(predictions)\n",
    "            prediction = torch.vstack(predictions).mean(0)\n",
    "        \n",
    "        label_idx  = prediction.argmax(-1).item()\n",
    "        return self.labels[label_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sazerlife/.conda/envs/torch=3.9/lib/python3.9/site-packages/torchaudio/transforms/_transforms.py:611: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n",
      "/home/sazerlife/.conda/envs/torch=3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sazerlife/.conda/envs/torch=3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "infer_wrapper = InferWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5683/5683 [02:07<00:00, 44.70it/s]\n"
     ]
    }
   ],
   "source": [
    "train_csv = pd.read_csv(train_csv_path)\n",
    "labels = list()\n",
    "\n",
    "for fname, _ in tqdm(train_csv.values):\n",
    "    label = infer_wrapper(train_audio_path / fname)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv['predicted'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05067745908850959, 0.01746065986170221)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_csv[\"label\"].values, train_csv[\"predicted\"].values), f1_score(train_csv[\"label\"].values, train_csv[\"predicted\"].values, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.051909202885799755, 0.01796734257852576)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_csv[\"label\"].values, train_csv[\"predicted\"].values), f1_score(train_csv[\"label\"].values, train_csv[\"predicted\"].values, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Squeak              2629\n",
       "Gong                2177\n",
       "Cello                322\n",
       "Trumpet              239\n",
       "Violin_or_fiddle     239\n",
       "Fart                  68\n",
       "Knock                  7\n",
       "Flute                  2\n",
       "Name: predicted, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv['predicted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv(test_csv_path)\n",
    "labels = list()\n",
    "\n",
    "for fname, in tqdm(test_csv.values):\n",
    "    label = infer_wrapper(test_audio_path / fname)\n",
    "    labels.append(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch=3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
